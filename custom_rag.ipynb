{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®‰è£…æ ¸å¿ƒä¾èµ–\n",
    "\n",
    "```bash\n",
    "# pip3 install sentence-transformers chromadb transformers torch numpy openai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================== æ¨¡å—1ï¼šEmbeddingæ¨¡å‹å°è£…ï¼ˆç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒçµæ´»æ‰©å±•ï¼‰ ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\soft\\py\\py_3_11_9\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æœ¬åœ°Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦ï¼š768\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai  # å¯é€‰ï¼šäº‘ç«¯Embeddingæ¨¡å‹ä¾èµ–\n",
    "\n",
    "# 1. å®šä¹‰EmbeddingæŠ½è±¡åŸºç±»ï¼ˆç»Ÿä¸€æ¥å£ï¼Œæ–°å¢æ¨¡å‹åªéœ€å®ç°è¯¥æ¥å£ï¼‰\n",
    "class EmbeddingBase(ABC):\n",
    "    @abstractmethod\n",
    "    def embed_text(self, texts: Union[str, List[str]], normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        æ–‡æœ¬å‘é‡åŒ–æ ¸å¿ƒæ–¹æ³•\n",
    "        :param texts: å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²æˆ–æ–‡æœ¬åˆ—è¡¨\n",
    "        :param normalize: æ˜¯å¦å½’ä¸€åŒ–å‘é‡ï¼ˆæå‡æ£€ç´¢ç›¸ä¼¼åº¦è®¡ç®—ç²¾åº¦ï¼‰\n",
    "        :return: å‘é‡æ•°ç»„ï¼ˆshape: [æ–‡æœ¬æ•°é‡, å‘é‡ç»´åº¦]ï¼‰\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def vector_dim(self) -> int:\n",
    "        \"\"\"è¿”å›æ¨¡å‹ç”Ÿæˆçš„å‘é‡ç»´åº¦ï¼ˆç”¨äºChromaå‘é‡åº“åˆå§‹åŒ–ï¼‰\"\"\"\n",
    "        pass\n",
    "\n",
    "# 2. æœ¬åœ°å¼€æºEmbeddingæ¨¡å‹å®ç°ï¼ˆä»¥BAAI/bge-base-zh-v1.5ä¸ºä¾‹ï¼Œä¸­æ–‡æ¯”è¾ƒä¼˜é€‰æ‹©ï¼‰\n",
    "class LocalBgeEmbedding(EmbeddingBase):\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-base-zh-v1.5\", device: str = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æœ¬åœ°Embeddingæ¨¡å‹\n",
    "        :param model_name: æ¨¡å‹åç§°ï¼ˆHugging Faceä»“åº“ï¼‰æˆ–æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "        :param device: è¿è¡Œè®¾å¤‡ï¼ˆNoneè‡ªåŠ¨é€‰æ‹©ï¼Œ\"cuda\"=GPUï¼Œ\"cpu\"=CPUï¼‰\n",
    "        \"\"\"\n",
    "        # åŠ è½½æ¨¡å‹ï¼ˆæ”¯æŒæœ¬åœ°è·¯å¾„/è¿œç¨‹ä»“åº“ï¼‰\n",
    "        self.model = SentenceTransformer(model_name, device=device)\n",
    "        self._vector_dim = None  # å»¶è¿Ÿåˆå§‹åŒ–å‘é‡ç»´åº¦\n",
    "\n",
    "    def embed_text(self, texts: Union[str, List[str]], normalize: bool = True) -> np.ndarray:\n",
    "        # å¤„ç†å•ä¸ªæ–‡æœ¬è¾“å…¥\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # ç”Ÿæˆå‘é‡ï¼šnormalize=Trueæ˜¯å…³é”®ä¼˜åŒ–ï¼ˆæå‡æ£€ç´¢æ•ˆæœï¼‰\n",
    "        embeddings = self.model.encode(\n",
    "            texts,\n",
    "            convert_to_numpy=True,  # è½¬ä¸ºnumpyæ•°ç»„\n",
    "            normalize_embeddings=normalize,  # å‘é‡å½’ä¸€åŒ–\n",
    "            show_progress_bar=False  # å…³é—­è¿›åº¦æ¡\n",
    "        )\n",
    "        \n",
    "        # è‡ªåŠ¨è·å–å‘é‡ç»´åº¦ï¼ˆä»…é¦–æ¬¡è°ƒç”¨æ—¶åˆå§‹åŒ–ï¼‰\n",
    "        if self._vector_dim is None:\n",
    "            self._vector_dim = embeddings.shape[1]\n",
    "        return embeddings\n",
    "\n",
    "    @property\n",
    "    def vector_dim(self) -> int:\n",
    "        # è‹¥æœªåˆå§‹åŒ–ï¼Œé€šè¿‡æµ‹è¯•æ–‡æœ¬è·å–å‘é‡ç»´åº¦\n",
    "        if self._vector_dim is None:\n",
    "            self.embed_text(\"æµ‹è¯•æ–‡æœ¬\")\n",
    "        return self._vector_dim\n",
    "\n",
    "# 3. å¯é€‰ï¼šäº‘ç«¯Embeddingæ¨¡å‹å®ç°ï¼ˆä»¥OpenAIä¸ºä¾‹ï¼Œæ— éœ€æœ¬åœ°ç®—åŠ›ï¼‰\n",
    "class OpenAIEmbedding(EmbeddingBase):\n",
    "    def __init__(self, api_key: str, model_name: str = \"text-embedding-3-small\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–OpenAI Embedding API\n",
    "        :param api_key: OpenAI APIå¯†é’¥\n",
    "        :param model_name: Embeddingæ¨¡å‹åç§°\n",
    "        \"\"\"\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.model_name = model_name\n",
    "        self._vector_dim = None\n",
    "\n",
    "    def embed_text(self, texts: Union[str, List[str]], normalize: bool = True) -> np.ndarray:\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # è°ƒç”¨OpenAI APIç”Ÿæˆå‘é‡\n",
    "        response = self.client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=self.model_name\n",
    "        )\n",
    "        \n",
    "        # æå–å‘é‡å¹¶è½¬ä¸ºnumpyæ•°ç»„\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        embeddings_np = np.array(embeddings)\n",
    "        \n",
    "        # å‘é‡å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼Œæå‡æ£€ç´¢ç²¾åº¦ï¼‰\n",
    "        if normalize:\n",
    "            embeddings_np = embeddings_np / np.linalg.norm(embeddings_np, axis=1, keepdims=True)\n",
    "        \n",
    "        # åˆå§‹åŒ–å‘é‡ç»´åº¦\n",
    "        if self._vector_dim is None:\n",
    "            self._vector_dim = embeddings_np.shape[1]\n",
    "        return embeddings_np\n",
    "\n",
    "    @property\n",
    "    def vector_dim(self) -> int:\n",
    "        if self._vector_dim is None:\n",
    "            self.embed_text(\"æµ‹è¯•æ–‡æœ¬\")\n",
    "        return self._vector_dim\n",
    "\n",
    "# åˆå§‹åŒ–æœ¬åœ°Embeddingæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œæ— éœ€API Keyï¼Œéšç§å®‰å…¨ï¼‰\n",
    "# è‹¥ç½‘ç»œæ— æ³•è®¿é—®Hugging Faceï¼Œå¯æ”¹ä¸ºæœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆå¦‚\"./local_models/bge-base-zh-v1.5\"ï¼‰\n",
    "local_embedding = LocalBgeEmbedding(device=None)\n",
    "print(f\"âœ… æœ¬åœ°Embeddingæ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‘é‡ç»´åº¦ï¼š{local_embedding.vector_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================== æ¨¡å—2ï¼šChromaå‘é‡æ•°æ®åº“å°è£…ï¼ˆåç«¯å­˜å‚¨å®ç°ï¼Œé€‚é…æ–°ç‰ˆChromaï¼‰ ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chromaå‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯ï¼š{'collection_name': 'my_custom_rag_kb', 'document_count': 20, 'vector_dim': 768, 'persist_directory': './chroma_persist'}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import os\n",
    "from typing import Optional, Dict\n",
    "\n",
    "class ChromaVectorStore:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: EmbeddingBase,\n",
    "        collection_name: str = \"rag_knowledge_base\",\n",
    "        persist_directory: str = \"./chroma_persist\",\n",
    "        metadata: Optional[Dict] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“ï¼ˆæŒä¹…åŒ–æ¨¡å¼ï¼Œæ–°ç‰ˆè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨persistï¼‰\n",
    "        :param embedding_model: å·²å®ä¾‹åŒ–çš„Embeddingæ¨¡å‹ï¼ˆç”¨äºå‘é‡ç”Ÿæˆï¼‰\n",
    "        :param collection_name: é›†åˆåç§°ï¼ˆå¯¹åº”ä¸€ä¸ªç‹¬ç«‹çŸ¥è¯†åº“ï¼‰\n",
    "        :param persist_directory: æŒä¹…åŒ–å­˜å‚¨ç›®å½•ï¼ˆæ•°æ®ä¿å­˜åœ¨æœ¬åœ°æ–‡ä»¶å¤¹ï¼‰\n",
    "        :param metadata: é›†åˆå…ƒæ•°æ®ï¼ˆå¦‚çŸ¥è¯†åº“æè¿°ï¼‰\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "\n",
    "        # 1. åˆå§‹åŒ–ChromaæŒä¹…åŒ–å®¢æˆ·ç«¯ï¼ˆæ–°ç‰ˆè‡ªåŠ¨æŒä¹…åŒ–ï¼Œæ— éœ€åç»­æ‰‹åŠ¨è°ƒç”¨persistï¼‰\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=persist_directory,\n",
    "            settings=Settings(\n",
    "                anonymized_telemetry=False,  # å…³é—­åŒ¿åæ•°æ®ä¸ŠæŠ¥\n",
    "                allow_reset=True  # å…è®¸é‡ç½®é›†åˆï¼ˆä»…æµ‹è¯•ç”¨ï¼‰\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2. è·å–æˆ–åˆ›å»ºé›†åˆï¼ˆç›¸å½“äºæ•°æ®åº“ä¸­çš„\"è¡¨\"ï¼‰\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            metadata=metadata or {\"description\": \"RAGæ¡†æ¶çŸ¥è¯†åº“é›†åˆ\"}\n",
    "        )\n",
    "\n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[str],\n",
    "        ids: Optional[List[str]] = None,\n",
    "        metadatas: Optional[List[Dict]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        å‘Chromaæ·»åŠ çŸ¥è¯†åº“æ–‡æ¡£ï¼ˆè‡ªåŠ¨ç”Ÿæˆå‘é‡å¹¶è‡ªåŠ¨æŒä¹…åŒ–ï¼‰\n",
    "        :param documents: æ–‡æœ¬æ–‡æ¡£åˆ—è¡¨ï¼ˆçŸ¥è¯†åº“å†…å®¹ï¼‰\n",
    "        :param ids: æ–‡æ¡£å”¯ä¸€IDåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰\n",
    "        :param metadatas: æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¦‚{\"source\": \"PDFæ–‡ä»¶1\"}ï¼‰\n",
    "        \"\"\"\n",
    "        # 1. ç”¨Embeddingæ¨¡å‹ç”Ÿæˆæ–‡æ¡£å‘é‡ï¼ˆè½¬ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œé€‚é…Chromaï¼‰\n",
    "        embeddings = self.embedding_model.embed_text(documents).tolist()\n",
    "\n",
    "        # 2. è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£IDï¼ˆè‹¥æœªæŒ‡å®šï¼‰\n",
    "        if ids is None:\n",
    "            current_count = self.collection.count()\n",
    "            ids = [f\"doc_{current_count + i + 1}\" for i in range(len(documents))]\n",
    "\n",
    "        # 3. æ·»åŠ æ–‡æ¡£ã€å‘é‡ã€å…ƒæ•°æ®åˆ°é›†åˆï¼ˆæ–°ç‰ˆChromaè‡ªåŠ¨æŒä¹…åŒ–åˆ°æœ¬åœ°ç›®å½•ï¼‰\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            embeddings=embeddings,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "        # æ–°ç‰ˆæ— éœ€æ‰‹åŠ¨è°ƒç”¨persist()ï¼Œåˆ é™¤è¿‡æ—¶ä»£ç \n",
    "        print(f\"âœ… æˆåŠŸæ·»åŠ {len(documents)}æ¡æ–‡æ¡£ï¼Œå½“å‰çŸ¥è¯†åº“æ€»æ–‡æ¡£æ•°ï¼š{self.collection.count()}\")\n",
    "\n",
    "    def similarity_search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = 3,\n",
    "        where: Optional[Dict] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        ç›¸ä¼¼æ–‡æœ¬æ£€ç´¢ï¼ˆæ ¸å¿ƒï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢æ‰¾åˆ°æœ€ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼‰\n",
    "        :param query: ç”¨æˆ·æŸ¥è¯¢è¯­å¥\n",
    "        :param top_k: è¿”å›æœ€ç›¸ä¼¼çš„kæ¡æ–‡æ¡£\n",
    "        :param where: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚{\"source\": \"ç”µå•†è®¢å•\"}ï¼‰\n",
    "        :return: æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼ˆå«æ–‡æ¡£ã€ç›¸ä¼¼åº¦ã€IDã€å…ƒæ•°æ®ï¼‰\n",
    "        \"\"\"\n",
    "        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡\n",
    "        query_embedding = self.embedding_model.embed_text(query).tolist()\n",
    "\n",
    "        # 2. Chromaç›¸ä¼¼æ€§æ£€ç´¢ï¼ˆè¿”å›è·ç¦»å’Œæ–‡æ¡£ä¿¡æ¯ï¼‰\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=top_k,\n",
    "            where=where,\n",
    "            include=[\"documents\", \"distances\", \"metadatas\"]  # æŒ‡å®šè¿”å›å­—æ®µ\n",
    "        )\n",
    "\n",
    "        # 3. æ ¼å¼åŒ–ç»“æœï¼ˆè·ç¦»è½¬ç›¸ä¼¼åº¦ï¼Œ0-1ä¹‹é—´ï¼Œæ•°å€¼è¶Šå¤§è¶Šç›¸ä¼¼ï¼‰\n",
    "        formatted_results = []\n",
    "        for i in range(top_k):\n",
    "            if i >= len(results[\"ids\"][0]):  # é˜²æ­¢ç»“æœä¸è¶³top_k\n",
    "                break\n",
    "            formatted_results.append({\n",
    "                \"id\": results[\"ids\"][0][i],\n",
    "                \"document\": results[\"documents\"][0][i],\n",
    "                \"similarity\": round(1 / (1 + results[\"distances\"][0][i]), 4),  # è·ç¦»è½¬ç›¸ä¼¼åº¦\n",
    "                \"metadata\": results[\"metadatas\"][0][i] if results[\"metadatas\"] else None\n",
    "            })\n",
    "        return formatted_results\n",
    "\n",
    "    def get_collection_stats(self) -> Dict:\n",
    "        \"\"\"è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            \"collection_name\": self.collection_name,\n",
    "            \"document_count\": self.collection.count(),\n",
    "            \"vector_dim\": self.embedding_model.vector_dim,\n",
    "            \"persist_directory\": self.persist_directory\n",
    "        }\n",
    "\n",
    "# åˆå§‹åŒ–Chromaå‘é‡å­˜å‚¨ï¼ˆæ³¨å…¥æœ¬åœ°Embeddingæ¨¡å‹ï¼Œæ–°ç‰ˆè‡ªåŠ¨æŒä¹…åŒ–ï¼‰\n",
    "chroma_store = ChromaVectorStore(\n",
    "    embedding_model=local_embedding,\n",
    "    collection_name=\"my_custom_rag_kb\",\n",
    "    persist_directory=\"./chroma_persist\"\n",
    ")\n",
    "print(f\"âœ… Chromaå‘é‡æ•°æ®åº“åˆå§‹åŒ–å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯ï¼š{chroma_store.get_collection_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================== æ¨¡å—3ï¼šå®Œæ•´RAGæ¡†æ¶æ•´åˆï¼ˆæ£€ç´¢+ç”Ÿæˆï¼‰ ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ æ­£åœ¨åŠ è½½LLMæ¨¡å‹ï¼šQwen/Qwen2-0.5B-Instructï¼Œè¿è¡Œè®¾å¤‡ï¼šcpu\n",
      "âœ… LLMæ¨¡å‹åŠ è½½å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class CustomRAGSystem:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: EmbeddingBase,\n",
    "        chroma_store: ChromaVectorStore,\n",
    "        llm_model_name: str = \"Qwen/Qwen2-0.5B-Instruct\"  # è½»é‡LLMï¼ŒCPUå¯è¿è¡Œ\n",
    "    ):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–å®Œæ•´RAGç³»ç»Ÿ\n",
    "        :param embedding_model: Embeddingæ¨¡å‹å®ä¾‹\n",
    "        :param chroma_store: Chromaå‘é‡å­˜å‚¨å®ä¾‹\n",
    "        :param llm_model_name: LLMæ¨¡å‹åç§°ï¼ˆHugging Faceï¼‰æˆ–æœ¬åœ°è·¯å¾„\n",
    "        \"\"\"\n",
    "        self.embedding_model = embedding_model\n",
    "        self.chroma_store = chroma_store\n",
    "\n",
    "        # è‡ªåŠ¨é€‰æ‹©è¿è¡Œè®¾å¤‡ï¼ˆæœ‰GPUç”¨GPUï¼Œæ— GPUç”¨CPUï¼‰\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"ğŸ“¥ æ­£åœ¨åŠ è½½LLMæ¨¡å‹ï¼š{llm_model_name}ï¼Œè¿è¡Œè®¾å¤‡ï¼š{self.device}\")\n",
    "\n",
    "        # 1. åˆå§‹åŒ–LLMåˆ†è¯å™¨ï¼ˆå°†æ–‡æœ¬è½¬ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„ç¼–ç ï¼‰\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            llm_model_name,\n",
    "            trust_remote_code=True,  # ä¿¡ä»»è‡ªå®šä¹‰æ¨¡å‹ä»£ç \n",
    "            padding_side=\"right\"  # å³ä¾§å¡«å……ï¼ˆä¼˜åŒ–ç”Ÿæˆæ•ˆæœï¼‰\n",
    "        )\n",
    "\n",
    "        # 2. åˆå§‹åŒ–LLMæ¨¡å‹ï¼ˆç”Ÿæˆå›ç­”çš„æ ¸å¿ƒï¼‰\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(\n",
    "            llm_model_name,\n",
    "            device_map=self.device,  # è‡ªåŠ¨åˆ†é…è®¾å¤‡\n",
    "            torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,  # é€‚é…ç²¾åº¦\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.llm.eval()  # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼ï¼ˆå…³é—­è®­ç»ƒç›¸å…³åŠŸèƒ½ï¼‰\n",
    "        print(f\"âœ… LLMæ¨¡å‹åŠ è½½å®Œæˆ\")\n",
    "\n",
    "    def load_knowledge_base(self, knowledge_texts: List[str], metadatas: Optional[List[Dict]] = None):\n",
    "        \"\"\"åŠ è½½çŸ¥è¯†åº“åˆ°Chromaå‘é‡åº“\"\"\"\n",
    "        self.chroma_store.add_documents(documents=knowledge_texts, metadatas=metadatas)\n",
    "\n",
    "    def _build_prompt(self, query: str, retrieved_docs: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        æ„å»ºRAGæç¤ºè¯ï¼ˆæ ¸å¿ƒï¼šå¼•å¯¼LLMåŸºäºçŸ¥è¯†åº“å›ç­”ï¼Œé¿å…å¹»è§‰ï¼‰\n",
    "        :param query: ç”¨æˆ·é—®é¢˜\n",
    "        :param retrieved_docs: æ£€ç´¢åˆ°çš„ç›¸ä¼¼æ–‡æ¡£\n",
    "        :return: æ ¼å¼åŒ–æç¤ºè¯\n",
    "        \"\"\"\n",
    "        # æ‹¼æ¥çŸ¥è¯†åº“å†…å®¹\n",
    "        context = \"\\n\\n\".join([f\"ã€çŸ¥è¯†åº“{idx+1}ã€‘ï¼š{doc['document']}\" for idx, doc in enumerate(retrieved_docs)])\n",
    "        \n",
    "        # æç¤ºè¯æ¨¡æ¿ï¼ˆæ¸…æ™°çº¦æŸLLMè¡Œä¸ºï¼Œæå‡å›ç­”è´¨é‡ï¼‰\n",
    "        prompt_template = \"\"\"\n",
    "è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦å¼•å…¥ä»»ä½•çŸ¥è¯†åº“å¤–çš„ä¿¡æ¯ã€‚\n",
    "å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å›å¤â€œæœªæ£€ç´¢åˆ°ç›¸å…³çŸ¥è¯†ï¼Œæ— æ³•å›ç­”â€ã€‚\n",
    "\n",
    "### çŸ¥è¯†åº“å†…å®¹ï¼š\n",
    "{context}\n",
    "\n",
    "### ç”¨æˆ·é—®é¢˜ï¼š\n",
    "{query}\n",
    "\n",
    "### å›ç­”ï¼š\n",
    "        \"\"\"\n",
    "        return prompt_template.format(context=context, query=query).strip()\n",
    "\n",
    "    def generate_answer(self, query: str, top_k: int = 3, temperature: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        ç”ŸæˆRAGå›ç­”ï¼ˆç«¯åˆ°ç«¯æµç¨‹ï¼šæ£€ç´¢â†’æ„å»ºæç¤ºè¯â†’LLMç”Ÿæˆï¼‰\n",
    "        :param query: ç”¨æˆ·é—®é¢˜\n",
    "        :param top_k: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£æ•°é‡\n",
    "        :param temperature: ç”Ÿæˆéšæœºæ€§ï¼ˆ0=ç¡®å®šæ€§å›ç­”ï¼Œ1=æœ€å¤§éšæœºæ€§ï¼‰\n",
    "        :return: æ ¼å¼åŒ–å›ç­”ç»“æœ\n",
    "        \"\"\"\n",
    "        # æ­¥éª¤1ï¼šç›¸ä¼¼æ–‡æ¡£æ£€ç´¢\n",
    "        retrieved_docs = self.chroma_store.similarity_search(query, top_k=top_k)\n",
    "        print(f\"ğŸ” æ£€ç´¢åˆ°{len(retrieved_docs)}æ¡ç›¸ä¼¼æ–‡æ¡£\")\n",
    "\n",
    "        # æ­¥éª¤2ï¼šæ„å»ºæç¤ºè¯\n",
    "        prompt = self._build_prompt(query, retrieved_docs)\n",
    "\n",
    "        # æ­¥éª¤3ï¼šLLMç”Ÿæˆå›ç­”\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)  # è½¬ä¸ºå¼ é‡å¹¶ç§»è‡³æŒ‡å®šè®¾å¤‡\n",
    "        with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—ï¼ŒèŠ‚çœæ˜¾å­˜/å†…å­˜\n",
    "            outputs = self.llm.generate(\n",
    "                **inputs,\n",
    "                temperature=temperature,\n",
    "                max_new_tokens=512,  # æœ€å¤§ç”Ÿæˆå­—æ•°\n",
    "                pad_token_id=self.tokenizer.eos_token_id,  # å¡«å……token\n",
    "                eos_token_id=self.tokenizer.eos_token_id  # ç»“æŸtoken\n",
    "            )\n",
    "\n",
    "        # æ­¥éª¤4ï¼šè§£æå›ç­”ï¼ˆå»é™¤æç¤ºè¯éƒ¨åˆ†ï¼Œåªä¿ç•™å›ç­”å†…å®¹ï¼‰\n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = full_text.split(\"### å›ç­”ï¼š\")[-1].strip()\n",
    "\n",
    "        # è¿”å›æ ¼å¼åŒ–ç»“æœ\n",
    "        return {\n",
    "            \"user_query\": query,\n",
    "            \"retrieved_documents\": retrieved_docs,\n",
    "            \"rag_answer\": answer,\n",
    "            \"top_k\": top_k\n",
    "        }\n",
    "\n",
    "# å®ä¾‹åŒ–å®Œæ•´RAGç³»ç»Ÿ\n",
    "rag_system = CustomRAGSystem(\n",
    "    embedding_model=local_embedding,\n",
    "    chroma_store=chroma_store,\n",
    "    llm_model_name=\"Qwen/Qwen2-0.5B-Instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================== æ¨¡å—4ï¼šæµ‹è¯•è¿è¡Œï¼ˆéªŒè¯RAGåŠŸèƒ½ï¼‰ ======================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸæ·»åŠ 5æ¡æ–‡æ¡£ï¼Œå½“å‰çŸ¥è¯†åº“æ€»æ–‡æ¡£æ•°ï¼š25\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ RAGé—®ç­”æµ‹è¯•å¼€å§‹\n",
      "============================================================\n",
      "ğŸ” æ£€ç´¢åˆ°2æ¡ç›¸ä¼¼æ–‡æ¡£\n",
      "\n",
      "ğŸ“ ç”¨æˆ·é—®é¢˜ï¼šBAAI/bge-base-zhé€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ\n",
      "ğŸ¤– RAGå›ç­”ï¼š\n",
      "ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼š2\n",
      "--------------------------------------------------\n",
      "ğŸ” æ£€ç´¢åˆ°2æ¡ç›¸ä¼¼æ–‡æ¡£\n",
      "\n",
      "ğŸ“ ç”¨æˆ·é—®é¢˜ï¼šChromaå‘é‡æ•°æ®åº“çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "ğŸ¤– RAGå›ç­”ï¼š\n",
      "ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼š2\n",
      "--------------------------------------------------\n",
      "ğŸ” æ£€ç´¢åˆ°2æ¡ç›¸ä¼¼æ–‡æ¡£\n",
      "\n",
      "ğŸ“ ç”¨æˆ·é—®é¢˜ï¼šRAGæ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤æœ‰å“ªäº›ï¼Ÿ\n",
      "ğŸ¤– RAGå›ç­”ï¼šRAGæ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤æœ‰ï¼š\n",
      "\n",
      "çŸ¥è¯†åº“1ï¼šRAGæ¡†æ¶çš„æ ¸å¿ƒä¸‰æ­¥åŒ…æ‹¬ï¼šçŸ¥è¯†åº“å‘é‡åŒ–å­˜å‚¨ã€ç”¨æˆ·æŸ¥è¯¢ç›¸ä¼¼æ£€ç´¢å’Œç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”ã€‚è¿™äº›æ­¥éª¤æ˜¯RAGæ¡†æ¶çš„ä¸‰ä¸ªå…³é”®ç¯èŠ‚ï¼Œå®ƒä»¬å…±åŒæ„æˆäº†RAGæ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½ã€‚\n",
      "\n",
      "çŸ¥è¯†åº“2ï¼šRAGæ¡†æ¶çš„æ ¸å¿ƒä¸‰æ­¥è¿˜åŒ…æ‹¬ï¼šç”¨æˆ·æŸ¥è¯¢ç›¸ä¼¼æ£€ç´¢ã€ç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”ä»¥åŠçŸ¥è¯†åº“å‘é‡åŒ–å­˜å‚¨ã€‚è¿™ä¸‰ä¸ªæ­¥éª¤æ„æˆäº†RAGæ¡†æ¶çš„åŸºæœ¬æ¡†æ¶ï¼Œå…¶ä¸­ï¼Œç”¨æˆ·æŸ¥è¯¢ç›¸ä¼¼æ£€ç´¢è´Ÿè´£æä¾›ä¸ç”¨æˆ·æœç´¢ç›¸å…³çš„ä¿¡æ¯ï¼›ç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”åˆ™æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢ç»“æœï¼Œä¸ºç”¨æˆ·æä¾›ä¸€ä¸ªæ›´å‡†ç¡®çš„ç­”æ¡ˆï¼›è€ŒçŸ¥è¯†åº“å‘é‡åŒ–å­˜å‚¨åˆ™ç”¨äºå°†å¤§é‡çš„çŸ¥è¯†æ•°æ®è¿›è¡Œè§„èŒƒåŒ–å¤„ç†ï¼Œä»¥ä¾¿æ›´å¥½åœ°æœåŠ¡äºç”¨æˆ·ã€‚å› æ­¤ï¼ŒRAGæ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤æ˜¯é€šè¿‡è¿™ä¸‰ä¸ªç¯èŠ‚ï¼Œå®ç°å¯¹å¤§é‡ä¿¡æ¯çš„é«˜æ•ˆç®¡ç†å’Œå¿«é€Ÿå“åº”ã€‚\n",
      "ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼š2\n",
      "--------------------------------------------------\n",
      "ğŸ” æ£€ç´¢åˆ°2æ¡ç›¸ä¼¼æ–‡æ¡£\n",
      "\n",
      "ğŸ“ ç”¨æˆ·é—®é¢˜ï¼šEmbeddingæ¨¡å‹çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "ğŸ¤– RAGå›ç­”ï¼šEmbeddingæ¨¡å‹æ˜¯ä¸€ç§ç”¨äºè¡¨ç¤ºæ–‡æœ¬çš„ç¨€ç–è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒä½¿ç”¨é‚»æ¥çŸ©é˜µæ¥æè¿°æ–‡æœ¬å’Œå®ƒçš„ä¸Šä¸‹æ–‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–‡æœ¬çœ‹ä½œæ˜¯ä¸€ä¸ªé«˜ç»´ç©ºé—´ï¼Œè€Œå…¶ä¸Šä¸‹æ–‡åˆ™è¢«å®šä¹‰ä¸ºä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªå…ƒç´ ï¼ˆå•è¯æˆ–å¥å­ï¼‰ï¼Œæ¯åˆ—ä»£è¡¨ä¸€ä¸ªç‰¹å¾å€¼ã€‚è¿™ç§åšæ³•ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚\n",
      "\n",
      "å› æ­¤ï¼ŒEmbeddingæ¨¡å‹çš„ä¸»è¦ä½œç”¨æ˜¯åœ¨æ–‡æœ¬å¤„ç†å’Œæœºå™¨ç¿»è¯‘ç­‰é¢†åŸŸæ‰®æ¼”ç€é‡è¦çš„è§’è‰²ï¼Œå®ƒå¯ä»¥å°†æ–‡æœ¬è½¬æ¢æˆæ›´æ˜“äºç†è§£å’Œè¡¨è¾¾çš„å½¢å¼ï¼Œå¹¶ä¸”å¯ä»¥ç”¨æ¥è¡¡é‡æ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚\n",
      "ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼š2\n",
      "--------------------------------------------------\n",
      "\n",
      "âœ… æµ‹è¯•å®Œæˆï¼ŒçŸ¥è¯†åº“ç»Ÿè®¡ï¼š{'collection_name': 'my_custom_rag_kb', 'document_count': 25, 'vector_dim': 768, 'persist_directory': './chroma_persist'}\n"
     ]
    }
   ],
   "source": [
    "# 1. å‡†å¤‡æµ‹è¯•çŸ¥è¯†åº“ï¼ˆå¯æ›¿æ¢ä¸ºä½ çš„ä¸šåŠ¡çŸ¥è¯†ï¼‰\n",
    "test_knowledge = [\n",
    "    \"BAAI/bge-base-zhæ˜¯åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢å‘å¸ƒçš„ä¸­æ–‡å¼€æºEmbeddingæ¨¡å‹ï¼Œå‘é‡ç»´åº¦768ï¼Œé€‚åˆä¸­æ–‡RAGæ£€ç´¢åœºæ™¯ã€‚\",\n",
    "    \"Chromaæ˜¯è½»é‡çº§æœ¬åœ°å‘é‡æ•°æ®åº“ï¼Œæ”¯æŒæŒä¹…åŒ–å­˜å‚¨å’Œå…ƒæ•°æ®è¿‡æ»¤ï¼Œæ— éœ€å¤æ‚éƒ¨ç½²ï¼Œé€‚åˆç§æœ‰åŒ–RAGæ¡†æ¶ã€‚\",\n",
    "    \"RAGæ¡†æ¶æ ¸å¿ƒä¸‰æ­¥ï¼šçŸ¥è¯†åº“å‘é‡åŒ–å­˜å‚¨ã€ç”¨æˆ·æŸ¥è¯¢ç›¸ä¼¼æ£€ç´¢ã€ç»“åˆæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”ã€‚\",\n",
    "    \"Embeddingæ¨¡å‹çš„ä½œç”¨æ˜¯å°†æ–‡æœ¬è½¬ä¸ºä½ç»´ç¨ å¯†å‘é‡ï¼Œé€šè¿‡ä½™å¼¦ç›¸ä¼¼åº¦åˆ¤æ–­æ–‡æœ¬è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚\",\n",
    "    \"Qwen2æ˜¯é˜¿é‡Œäº‘å‘å¸ƒçš„è½»é‡çº§å¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡æµç•…ç”Ÿæˆï¼Œé€‚åˆæœ¬åœ°ä½ç®—åŠ›è®¾å¤‡è¿è¡Œã€‚\"\n",
    "]\n",
    "\n",
    "# 2. åŠ è½½çŸ¥è¯†åº“åˆ°Chroma\n",
    "rag_system.load_knowledge_base(\n",
    "    knowledge_texts=test_knowledge,\n",
    "    metadatas=[{\"source\": \"æµ‹è¯•çŸ¥è¯†åº“\"} for _ in test_knowledge]\n",
    ")\n",
    "\n",
    "# 3. æµ‹è¯•ç”¨æˆ·æŸ¥è¯¢\n",
    "user_queries = [\n",
    "    \"BAAI/bge-base-zhé€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ\",\n",
    "    \"Chromaå‘é‡æ•°æ®åº“çš„ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"RAGæ¡†æ¶çš„æ ¸å¿ƒæ­¥éª¤æœ‰å“ªäº›ï¼Ÿ\",\n",
    "    \"Embeddingæ¨¡å‹çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ\"\n",
    "]\n",
    "\n",
    "# 4. æ‰¹é‡ç”Ÿæˆå›ç­”å¹¶è¾“å‡º\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ¯ RAGé—®ç­”æµ‹è¯•å¼€å§‹\")\n",
    "print(\"=\"*60)\n",
    "for query in user_queries:\n",
    "    result = rag_system.generate_answer(query, top_k=2)\n",
    "    print(f\"\\nğŸ“ ç”¨æˆ·é—®é¢˜ï¼š{result['user_query']}\")\n",
    "    print(f\"ğŸ¤– RAGå›ç­”ï¼š{result['rag_answer']}\")\n",
    "    print(f\"ğŸ“Š æ£€ç´¢æ–‡æ¡£æ•°é‡ï¼š{len(result['retrieved_documents'])}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# 5. è¾“å‡ºçŸ¥è¯†åº“æœ€ç»ˆç»Ÿè®¡\n",
    "print(f\"\\nâœ… æµ‹è¯•å®Œæˆï¼ŒçŸ¥è¯†åº“ç»Ÿè®¡ï¼š{chroma_store.get_collection_stats()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
